---
title: "PSTAT131 Homework 3"
author: "Shivani Kharva"
date: "2022-10-17"
output: html_document
---

# Homework 3

### Load the Data/ Packages

```{r}
titanic_data <- read.csv("data/titanic.csv")
library(tidymodels)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
tidymodels_prefer()
```

```{r}
# Changing `survived` and `pclass` to factors
survived_levels <- c("Yes", "No")

titanic_data$survived <- factor(titanic_data$survived, levels = survived_levels)
titanic_data$pclass <- as.factor(titanic_data$pclass)

# Setting the seed
set.seed(0124)

# Viewing the data
head(titanic_data)
```

### Question 1

```{r}
# Splitting the data and stratifying on the outcome, `survived`
titanic_split <- initial_split(titanic_data, prop = 0.70, strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```

```{r}
# Verifying that the training and testing data have the correct number of outcomes
nrow(titanic_train)/nrow(titanic_data)
nrow(titanic_test)/nrow(titanic_data)
```

The training data has \~70% of the observations of the original data set and the testing data has \~30% of the observations of the original data set.

```{r}
# Looking at a general summary of the data
titanic_train %>% 
  summary()
```

The summary indicates that there is missing data in the training data in the `age` column (124 NA values). It is significant that `age` has missing data because we plan to use `age` as a predictor in our model.

Also, the summary reveals that there are more individuals who did not survive than those who survived. This may point to the potential issue of the accuracy not being a very good measure of the data since, if the model defaults to 'No', its accuracy might still be high because there are more data points of individuals who did not survive than those who did survive.  

**Why is it a good idea to use stratified sampling for this data?**   
It is a good idea to use stratified sampling for this data because there are more people who did not survive than those who did. However, we still want to train the model equally on 'yes' and 'no' and stratified sampling allows us to do that. Stratifying on `survived` makes sure the distribution of `survived` in the training set is the same as the distribution of `survived` in the testing set. Therefore, it is a good idea to use stratified sampling for this data.  

### Question 2  

```{r}
survived_dist <- ggplot(titanic_train, aes(survived)) +
  geom_bar()
survived_dist
```

The two possible values of the outcome variable `survived` are 'No' and 'Yes' as to whether or not the passenger survived. There are more individuals who did not survived (384) than individuals who did survive (239), which was also shown in the summary from the previous question.

### Question 3  

```{r}
library(corrr)

# Creating a visualization of the correlation matrix
cor_titanic_train <- titanic_train %>% 
  select(-c('passenger_id', 'survived', 'pclass', 'name', 'sex', 'ticket', 'cabin', 'embarked')) %>% 
  correlate()
cor_titanic_train

# Heat map version of the visualization
cor_titanic_train_heatmap <- cor_titanic_train %>% 
  stretch() %>% 
  ggplot(aes(x, y, fill=r)) +
    geom_tile() +
    geom_text(aes(label = as.character(fashion(r))))
cor_titanic_train_heatmap
```

It appears that none of the predictors have a very strong correlation with one another since the highest correlation is between `parch` & `sib_sp` (\~0.419). The predictors with a positive correlation are `age` & `fare` (\~0.075), `sib_sp` & `parch` (\~0.419), `sib_sp` & `fare` (\~0.175), and `parch` & `fare` (\~0.223). The predictors with a negative correlation are `age` & `sib_sp` (\~-0.309) and `age` & `parch` (\~-0.209).

### Question 4

```{r}
# Creating a recipe using the specified predictors
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>% 
  # Using imputation to deal with missing age values
  step_impute_linear(age, impute_with = imp_vars(pclass, sex, sib_sp, parch, fare)) %>% 
  # Encoding categorical predictors
  step_dummy(all_nominal_predictors()) %>% 
  # Including interactions
  step_interact(terms = ~ starts_with("sex"):fare + age:fare)
```

### Question 5

```{r}
# Specifying logistic regression model for classification using glm engine
log_reg <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")

# Creating workflow
log_workflow <- workflow() %>% 
  # Adding the model
  add_model(log_reg) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
log_fit <- fit(log_workflow, titanic_train)
```

### Question 6

```{r}
# Specifying linear discriminant analysis model using MASS engine
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# Creating workflow
lda_workflow <- workflow() %>% 
  # Adding the model
  add_model(lda_mod) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
lda_fit <- fit(lda_workflow, titanic_train)
```

### Question 7

```{r}
# Specifying quadratic discriminant analysis model using MASS engine
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# Creating workflow
qda_workflow <- workflow() %>% 
  # Adding the model
  add_model(qda_mod) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
qda_fit <- fit(qda_workflow, titanic_train)
```

### Question 8

```{r}
# Specifying naive Bayes model for classification using klaR engine
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE)

# Creating workflow
nb_workflow <- workflow() %>% 
  # Adding model
  add_model(nb_mod) %>% 
  # Adding recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
nb_fit <- fit(nb_workflow, titanic_train)
```

### Question 9

```{r, message=FALSE, warning=FALSE}
# Generating predictions for logistic regression
log_tibble <- predict(log_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
log_tibble <- bind_cols(log_tibble, titanic_train %>% 
                              select(survived))

# Generating predictions for LDA
lda_tibble <- predict(lda_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
lda_tibble <- bind_cols(lda_tibble, titanic_train %>% 
                              select(survived))

# Generating predictions for QDA
qda_tibble <- predict(qda_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
qda_tibble <- bind_cols(qda_tibble, titanic_train %>% 
                              select(survived))

# Generating predictions for Naive Bayes
nb_tibble <- predict(nb_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
nb_tibble <- bind_cols(nb_tibble, titanic_train %>% 
                              select(survived))
```

```{r, message=FALSE, warning=FALSE}
log_reg_acc <- augment(log_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)
lda_acc <- augment(lda_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)
qda_acc <- augment(qda_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)
nb_acc <- augment(nb_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)

accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```

Logistic regression achieved the highest accuracy, 0.8138042, on the training data.

### Question 10

```{r}
# Fitting logistic regression to testing data
log_test_tibble <- predict(log_fit, new_data = titanic_test %>% 
                        select(-survived), type = "prob")
log_test_tibble <- bind_cols(log_test_tibble, titanic_test %>% 
                              select(survived))
```

```{r}
# Accuracy of model on testing data
log_reg_test_acc <- augment(log_fit, new_data = titanic_test) %>% 
  accuracy(truth = survived, estimate = .pred_class)

results2 <- tibble(model = "Logistic Regression", accuracy = log_reg_test_acc$.estimate)
results2
```

The logistic regression has an accuracy of 0.7985075 on the testing data.

```{r}
# Creating a confusion matrix
log_confusion_matrix <- augment(log_fit, new_data = titanic_test) %>% 
  conf_mat(truth = survived, estimate = .pred_class)
log_confusion_matrix

# Visualizing the confusion matrix
log_confusion_matrix_viz <- augment(log_fit, new_data = titanic_test) %>% 
  conf_mat(truth = survived, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
log_confusion_matrix_viz
```

```{r}
# Plotting ROC curve
log_roc <- augment(log_fit, new_data = titanic_test) %>% 
  roc_curve(survived, .pred_Yes) %>% 
  autoplot()
log_roc

# Finding AUC
log_test_tibble %>% 
  roc_auc(survived, .pred_Yes)
```

**How did the model perform?**

Based on the ROC curve and the AUC, the logistic regression model performed relatively well. Since the AUC is 0.8616946, the model does fairly well at discriminating between classes (whether a person survived or not). Also the ROC curve is curved upward towards the top left, which explains the high AUC.

**Compare its training and testing accuracies. If the values differ, why do you think this is so?**

The training accuracy of the logistic regression model was 0.8138042 and the testing accuracy of the model was 0.7985075. The values do not differ greatly, but the testing accuracy is less than the training accuracy. This is because the model was trained on the training data and is more used to those values; however, the testing data renders a lower accuracy because the model was not trained on those values so this accuracy reflects its performance on a completely new set of values that it does not already know.

