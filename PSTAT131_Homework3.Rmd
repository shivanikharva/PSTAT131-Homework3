---
title: "PSTAT131 Homework 3"
author: "Shivani Kharva"
date: "2022-10-17"
output: html_document
---

# Homework 3

### Load the Data/ Packages

```{r}
titanic_data <- read.csv("data/titanic.csv")
library(tidymodels)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
tidymodels_prefer()
```

```{r}
# Changing `survived` and `pclass` to factors
survived_levels <- c("Yes", "No")

titanic_data$survived <- factor(titanic_data$survived, levels = survived_levels)
titanic_data$pclass <- as.factor(titanic_data$pclass)

# Setting the seed
set.seed(0124)
head(titanic_data)
```

### Question 1

```{r}
# Splitting the data and stratifying on the outcome, `survived`
titanic_split <- initial_split(titanic_data, prop = 0.70, strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- training(titanic_split)
```

```{r}
# Verifying that the training and testing data have the correct number of outcomes
nrow(titanic_train)/nrow(titanic_data)
nrow(titanic_test)/nrow(titanic_data)
```

Both the training and testing data have ~70% of the observations of the original data set.

```{r}
# Checking if there are any missing values in the training data
colSums(is.na(titanic_train))
```

There is missing data in the training data (specifically in the `age`, `cabin`, and `embarked` columns). It is a good idea to use stratified sampling for this data so that the amount of missing data in the training and testing data is random and not specifically more in one data set or the other.  

```{r}
titanic_train %>% 
  summary()
```

### Question 2  

```{r}
survived_dist <- ggplot(titanic_train, aes(survived)) +
  geom_bar()
survived_dist
```

The two possible values of the outcome variable `survived` are 'No' and 'Yes' as to whether or not the passenger survived. There are more individuals who did not survived (~380) than individuals who did survive (~240).  

### Question 3  

```{r}
# Creating a correlation matrix of all continuous variables with the training data set
titanic_cor_matrix <- titanic_train %>% 
  select(-c('passenger_id', 'survived', 'pclass', 'name', 'sex', 'ticket', 'cabin', 'embarked')) %>% 
  cor()
titanic_cor_matrix
```

```{r}
library(corrr)

# Creating a visualization of the correlation matrix
cor_titanic_train <- titanic_train %>% 
  select(-c('passenger_id', 'survived', 'pclass', 'name', 'sex', 'ticket', 'cabin', 'embarked')) %>% 
  correlate()
cor_titanic_train
rplot(cor_titanic_train)

# Heat map version of the visualization
cor_titanic_train_heatmap <- cor_titanic_train %>% 
  stretch() %>% 
  ggplot(aes(x, y, fill=r)) +
    geom_tile() +
    geom_text(aes(label = as.character(fashion(r))))
cor_titanic_train_heatmap
```

It appears that all of the variables have a positive correlation with one another. Most of the correlations are not that strong as `fare` & `parch` only have a correlation of about 0.22 and `fare` & `sib_sp` only have a correlation of about 0.18. We can see that `sib_sp` and `parch` have the greatest correlation and it is only about 0.42.  

### Question 4  

```{r}
# Creating a recipe using the specified predictors
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_data) %>% 
  # Using imputation to deal with missing age values
  step_impute_linear(age, impute_with = imp_vars(pclass, sex, sib_sp, parch, fare)) %>% 
  # Encoding categorical predictors
  step_dummy(all_nominal_predictors()) %>% 
  # Including interactions
  step_interact(terms = ~ starts_with("sex"):fare + age:fare)
```

### Question 5  

```{r}
# Specifying logistic regression model for classification using glm engine
log_reg <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")

# Creating workflow
log_workflow <- workflow() %>% 
  # Adding the model
  add_model(log_reg) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
log_fit <- fit(log_workflow, titanic_train)
```

### Question 6  

```{r}
# Specifying linear discriminant analysis model using MASS engine
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# Creating workflow
lda_workflow <- workflow() %>% 
  # Adding the model
  add_model(lda_mod) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
lda_fit <- fit(lda_workflow, titanic_train)
```

### Question 7  

```{r}
# Specifying quadratic discriminant analysis model using MASS engine
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# Creating workflow
qda_workflow <- workflow() %>% 
  # Adding the model
  add_model(qda_mod) %>% 
  # Adding the recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
qda_fit <- fit(qda_workflow, titanic_train)
```

### Question 8  

```{r}
# Specifying naive Bayes model for classification using klaR engine
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE)

# Creating workflow
nb_workflow <- workflow() %>% 
  # Adding model
  add_model(nb_mod) %>% 
  # Adding recipe
  add_recipe(titanic_recipe)

# Using fit() to apply workflow to training data and storing it
nb_fit <- fit(nb_workflow, titanic_train)
```

### Question 10  

```{r}
# Generating predictions for logistic regression
log_tibble <- predict(log_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
log_tibble <- bind_cols(log_tibble, titanic_train %>% 
                              select(survived))

# Generating predictions for LDA
lda_tibble <- predict(lda_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
lda_tibble <- bind_cols(lda_tibble, titanic_train %>% 
                              select(survived))

# Generating predictions for QDA
qda_tibble <- predict(qda_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
qda_tibble <- bind_cols(qda_tibble, titanic_train %>% 
                              select(survived))

# Generating predictions for Naive Bayes
nb_tibble <- predict(nb_fit, new_data = titanic_train %>% 
                        select(-survived), type = "class")
nb_tibble <- bind_cols(nb_tibble, titanic_train %>% 
                              select(survived))
```

```{r}
log_reg_acc <- augment(log_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)
lda_acc <- augment(lda_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)
qda_acc <- augment(qda_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)
nb_acc <- augment(nb_fit, new_data = titanic_train) %>% 
  accuracy(truth = survived, estimate = .pred_class)

accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```

Logistic regressoin achieved the highest accuracy on the training data.  

### Question 10  

```{r}

```











